test -e "$REPO_PIGLIT/piglit" || return 1

function __piglit_gen_report__ {
    # generate a report, first print the passrate on the first line, then print the individual results
    python3 - 2> ${run_log_file}.processing_stderr << END
import traceback
import sys
import six

sys.path.append("$PIGLIT_RUN_PARAMS")
from framework import summary, status, core, backends, exceptions
from framework.programs import parsers

tests=list()
pass_count = 0

try:
    testrun = backends.load("${piglit_output}")

    for name, result in six.iteritems(testrun.tests):
        for key, value in six.iteritems(result.subtests):
            tests.append("{}<{}>: {}".format(name, key, value))
            if value == 'pass':
                pass_count += 1
        tests.append("{}: {}".format(name, result.result))
        if result.result == 'pass':
            pass_count += 1
except Exception as e:
    traceback.print_exc(file=sys.stderr)

if len(tests) > 0:
    print("{:.3f}".format(pass_count / len(tests)))
else:
    print("0")
for test in tests:
    print(test)
END

    # Go through all the subtests we wanted to get and check if all of them
    # are present. If not, mark them as missing.
    for subtest in "${run_sub_tests[@]}"; do
        grep "$subtest" "$run_log_file" 2> /dev/null > /dev/null
        [ $? -eq 1 ] && echo "$subtest: missing"
    done

    # Display the final status, if it was a full run
    if [ -z "${testSubTests[$t]}" ]; then
        if [ "$exit_code" -eq 0 ]; then
            echo ": completed"
        else
            echo ": incomplete"
        fi
    fi

    # delete the temporary report
    rm -rf ${piglit_output} 2>&1
}

function __piglit_resume__ {
    cd "$REPO_PIGLIT"

    # ignore the incomplete result to avoid being stuck in a loop
    local cmdline="$REPO_PIGLIT/piglit resume -n ${run_log_file}_tmp"

    # Start the debug console
    if hash xterm; then
        xterm -geometry +0+300 -e "echo $cmdline; tail -F \"$run_log_file.stdout\""&
        xterm_pid=$!
    fi

    ENV_DUMP_RESTRICT_TO_BINARY="NO_ENVDUMP_PLEASE"
    $cmdline >> "$run_log_file.stdout" 2>> "$run_log_file.stderr"
    local exit_code=$?

    # Kill the debug console
    [ -n "$xterm_pid" ] && kill $xterm_pid

    if [ $exit_code -ne 0 ]; then
        __piglit_resume__
    else
        __piglit_gen_report__
    fi
}

function __piglit_run__ {
    cd "$REPO_PIGLIT"

    local test_name=${testNames[$t]}
    local backend=$1
    local testscript=$2

    # Sub tests
    local has_subtests=0
    local testlist=''
    testlistfile="${run_log_file}.testlist"
    rm "$testlistfile"
    for subtest in "${run_sub_tests[@]}"; do
        echo "$subtest" | cut -d '<' -f 1 >> $testlistfile
        has_subtests=1
    done
    [ $has_subtests -eq 1 ] && testlist="--test-list $testlistfile"

    # start piglit. Use the sync mode to make the resume more reliable
    piglit_output=${run_log_file}_tmp
    local cmdline="$REPO_PIGLIT/piglit run -p $backend $PIGLIT_RUN_PARAMS -s $testlist $testscript ${piglit_output}"

    # Start the debug console
    if hash xterm; then
        xterm -geometry +0+300 -e "echo $cmdline; tail -F \"$run_log_file.stdout\""&
        xterm_pid=$!
    fi

    ENV_DUMP_REQUIRE_ARGUMENT="$REPO_PIGLIT/piglit" \
    ENV_DUMP_NO_METRICS=1 \
    run_bench 0 $cmdline > /dev/null 2> /dev/null
    local exit_code=$?

    # Kill the debug console
    [ -n "$xterm_pid" ] && kill $xterm_pid

    if [ $exit_code -ne 0 ]; then
        __piglit_resume__
    else
        __piglit_gen_report__
    fi
}

backends=$($REPO_PIGLIT/piglit run -h | grep "^  -p" | cut -d '{' -f 2 | cut -d '}' -f 1 | tr ',' ' ')
for backend in $backends; do
    for test_script in $REPO_PIGLIT/tests/*.py; do
        [ "$(basename $test_script)" == "__init__.py" ] && continue
        [ "$(basename $test_script)" == "igt.py" ] && continue

        name="piglit:$backend:$(basename ${test_script} | cut -d '.' -f 1)"
        eval "${name}_run() { __piglit_run__ $backend $test_script \$@; }"
        test_name="$test_name $name"
    done
done

test_unit="pass/total"
test_type="unit"
test_exec_time=600
